


## 最小二乘法



对于线性模型 $y = X\beta + \epsilon$

$$S(\beta) = \sum_{i=1}^n (y_i - X_i\beta)^2 = (y - X\beta)^T (y - X\beta)$$

目标函数展开：
 $$
S(\beta) = y^T y - 2\beta^T X^T y + \beta^T X^T X \beta
 $$
求导并求解：
 
对 \beta 求导并令导数为零，得到正规方程：
 $$
X^T X \beta = X^T y
 $$
若 X^T X 可逆，解为：
 $$
\hat{\beta} = (X^T X)^{-1} X^T y
 $$

解析解
$$
X\beta=y\\ (X^T X\beta = X^T y\\ \beta = (X^T X)^{-1} X^T y
$$
理想情况下方程组应该对数据微小的扰动具有抗噪性，保证其解的大体一致。

一般用条件数来衡量矩阵的病态程度，条件数定义为：矩阵的范数，乘以其逆矩阵的范数，即：
$$
cond ⁡ ( A ) = ∥ A ∥ ⋅ ∥ A − 1 ∥ \operatorname{cond}(A)=\|A\| \cdot\left\|A^{-1}\right\|
cond(A)=∥A∥⋅ 
∥
∥

 A ^−1
 
∥
∥
​$$
 
岭回归解为：
$$
θ = ( X T X + λ I ) − 1 X T y \theta=\left(X^{T} X+\lambda I\right)^{-1} X^{T} y
θ=(X 
T
 X+λI) 
−1
 X 
T
 y
$$
其中λ λλ是一个超参数，称为岭系数，I II是单位矩阵（由于对角线全是1，看起来像 “山岭”，由此得名岭回归）。

### 最小二乘准则：误差的平方和最小 估计出来的模型是最接近真实情形的(误差=真实值-理论值)

误差符合高斯分布

统计性质

最佳线性无偏估计（BLUE）：在高斯-马尔可夫假设下（零均值、同方差、无自相关误差），最小二乘估计量是最小方差的无偏估计。
 
最大似然估计：当误差服从正态分布时，最小二乘估计等价于最大似然估计。

误差项ϵ \epsilonϵ应当满足 ϵ ∈ N ( 0 , σ 2 ) \epsilon \in N\left(0, \sigma^{2}\right)ϵ∈N(0,σ 
2
 )，则每个观测样本y i y_{i}y 
i
​
 应该有：y i ∈ N ( f θ ( x i ) , σ 2 ) y_{i} \in N\left(f_{\theta}(x_i), \sigma^{2}\right)y 
i
 ∈N(f 
θ
 (x 
i
 ),σ 
2
 )，即观察到的样本 y i y_{i}y 
i
 是由理论值（模型的真实输出） f θ ( x i ) f_{\theta}(x_i)f 
θ(x i ) 叠加上高斯噪声 ϵ \epsilonϵ 得到的

1. 联合似然函数
假设样本独立，联合概率密度为各样本概率的乘积：
 $$
\mathcal{L}(\theta) = \prod_{i=1}^n p(y_i | x_i; \theta) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma} \exp\left(-\frac{(y_i - f_\theta(x_i))^2}{2\sigma^2}\right).
 $$
 
2. 对数似然函数
取自然对数简化计算：
 $$
\ln \mathcal{L}(\theta) = \sum_{i=1}^n \left -\ln(\sqrt{2\pi}\sigma) - \frac{(y_i - f_\theta(x_i))^2}{2\sigma^2} \right.
 $$
忽略常数项（与 \theta 无关的项）后，化简为：
 $$
\ln \mathcal{L}(\theta) \propto -\frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - f_\theta(x_i))^2.
 $$
 
3. 极大似然估计的目标
最大化对数似然等价于最小化负对数似然，即：
 即极大化似然函数等价于极小化最小二乘法的代价函数，这也表明了以误差平方和作为最佳拟合准则的合理性。

\hat{\theta}_{\text{MLE}} = \arg\min_\theta \sum_{i=1}^n (y_i - f_\theta(x_i))^2.
 
结论：在高斯噪声假设下，极大似然估计等价于最小化残差平方和，即 最小二乘准则。

在极大似然估计理论下，使用这些特定的损失函数训练出来的模型在理论上就是最优的。
